\subsection{Summary of model results}
\begin{table}
\input{tables/table_assumptions.tex}
\caption{A summary of the foundational assumptions of each method. Columns in the table map onto the lists of assumptions at the conclusions of each Concepts and Assumptions section above. UC: univariate contrast; SL: searchlight; R: ridge regression; L: LASSO; SOS: SOS LASSO.}
\label{tab.assumptions}
\end{table}

\begin{table}
\input{tables/table_modelresults.tex}
\caption{A summary of model results. Methods are labels by the same convention as in Table \ref{tab.assumptions}. SI/O stands for systematic input and output. ``Indicates'' means that the values attributed to identified units indicate how information of interest is coded across those units. ``Needs local'' indicates that the method is much less effective when information of interest is anatomically dispersed. $^*$Although ridge regression does identify all the systematic units, it also identifies all the arbitrary units. This contributes to the difficulties with interpreting these weights.}
\label{tab.modelresults}
\end{table}

Table \ref{tab.assumptions} summarizes the assumptions underlying each statistical approach, while Table \ref{tab.modelresults} summarizes how each approach answers the four core questions. Though unsurprising, it is still worth noting that each method succeeds best when the implicit assumptions it adopts are met in the data. Thus univariate contrast does exceedingly well identifying the I/O units, which in these simulations are always localized in the same way within and across individuals, and which adopt a consistent representational code within regions and across individuals. The SH units, which encode cleaner domain representations in a distributed manner, violate all of these assumptions and so are completely missed. Searchlight does well at detecting both systematic I/O and SH units, so long as the useful information is contained within the radius of a searchlight. If the searchlight is too small or too large relative to the anatomical distribution of the signal, the assumptions are violated and the method fails. LASSO performs modestly well at discovering SH units, but in assuming no consistency of any kind across model individuals, becomes highly susceptible to noise and so misses many important units. Ridge regression, in assuming highly redundant signal, spreads weights over all units, making the solution hard to decipher.

\soslasso succeeds best at finding distributed internal representations because it adopts assumptions that align well with PDP. Like other MVPA methods, no consistency in coding is assumed across individuals, allowing for differences in how the same representational structure is expressed over hidden units. In contrast to LASSO and ridge regression, representational units are assumed to be localized loosely within individuals, and with loose consistency of location across individuals, allowing for some degree of similarity in the global architecture. In contrast to univariate and searchlight methods, however, there is no assumption that all units within a given locale code the same representation, or that an informative region must contain sufficient information for a representation. This allows for the possibility that representations may be coded across multiple anatomically distal regions, and that the specific anatomical arrangement of the interesting elements can vary across individuals. Finally, the representation is assumed to be sparse, reflecting the notion that the representational elements of interest will typically be buried within a large system, with many measured components likely to be subserving unrelated functions. With these assumptions implemented in a whole-brain optimization, the method succeeds well in finding the distributed representational structure that is central to cognition under the PDP view, but most difficult to find with other methods.
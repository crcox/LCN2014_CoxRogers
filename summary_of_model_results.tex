\subsection{Summary of model results}
\begin{table}
\input{tables/table_modelresults.tex}
\caption{A summary of the answers for each method across the four central questions. $\dagger$ The success of the searchlight was contingent on model parameters that may be difficult to discern in practice. $\ddagger$ Ridge regression identifies all units as informative, including ones that in fact contain no information.} 
\label{tab.modelresults}
\end{table}
\begin{center}
	\textbf{---Figure \ref{tab.modelresults} about here---}
\end{center}


%Table \ref{tab.assumptions} summarizes the assumptions underlying each statistical approach, while Table \ref{tab.modelresults} summarizes how each approach answers the four core questions. 
Though unsurprising, it is nonetheless worth noting that each method succeeds best when the implicit assumptions it adopts are met in the data. Thus univariate contrast does exceedingly well identifying the I/O units, which in these simulations are always localized in the same way within and across individuals, and which adopt a consistent and independent representational code within regions and across individuals. The SH units, which encode cleaner domain representations in a distributed manner, violate all of these assumptions and so are completely missed. Searchlight does well at detecting both systematic I/O and SH units, but only if the useful information is contained within the radius of a searchlight and is localized in the same way across individuals. LASSO performs modestly well at discovering SH units, but in assuming no consistency of any kind across model individuals, becomes highly susceptible to noise and so misses many important units. Ridge regression, in assuming highly redundant signal, spreads weights over all units, making the solution hard to decipher.

\soslasso succeeds best at finding distributed internal representations because it adopts assumptions that align well with PDP. Like other MVPA methods, no consistency in coding is assumed across individuals, allowing for differences in how the same representational structure is expressed over hidden units. In contrast to LASSO and ridge regression, representational units are assumed to be localized loosely within individuals, and with loose consistency of location across individuals, allowing for some degree of similarity in the global architecture. In contrast to univariate and searchlight methods, however, there is no assumption that all units within a given locale code the same representation, or that an informative region must contain sufficient information for a representation. This allows for the possibility that representations may be coded across multiple anatomically distal regions, and that the specific anatomical arrangement of the interesting elements can vary across individuals. Finally, the representation is assumed to be sparse, reflecting the notion that the representational elements of interest will typically be buried within a large system, with many measured components likely to be subserving unrelated functions. With these assumptions implemented in a whole-brain optimization, the method succeeds well in finding the distributed representational structure that is central to cognition under the PDP view, but most difficult to find with other methods.


\documentclass[12pt,man,draftfirst]{apa6}
\usepackage[nodoi]{apacite}
\bibliographystyle{apacite}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
% \geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry

% See the ``Article customise'' template for come common customisations

\title{Taking distributed representations seriously}
\shorttitle{Taking distributed representations seriously}
\author{Christopher R. Cox\\Timothy T. Rogers}
\affiliation{University of Wisconsin, Madison}

% \date{} % delete this line to display the current date
\abstract{The Parallel Distributed Processing (PDP) approach to cognition assumes that active mental representations are distributed over many neural populations. It is known that distributed representations can be acquired through domain general learning mechanisms to economically encode graded similarity structure that supports generalization and is robust to disruption. Until recently, however, the PDP hypothesis has not strongly influenced functional  brain imaging, which has, for historical and methodological reasons, tended to adopt modular 
assumptions about how the brain encodes information. Multivoxel pattern analysis (MVPA) relaxes these assumptions, but the most common methods are only sensitive to quite localized similarity structure, either within a searchlight or predetermined regions of interest. In this paper we leverage a recent innovation in multivariate analysis, the ``sparse overlapping sets Lasso'' (SOSLasso), to seriously consider whether distributed representations like those that arise in PDP models are also employed in the brain.By applying SOSLasso and univariate methods to data generated by artificial neural networks where the representational structure is known, we show how and why reliable univariate results can systematically miss important distributed patterns where SOSLasso preferentially identifies such patterns. We then apply both methods to real brain imaging data, and show that, at least in some domains of interest, the underlying representations appear to be distributed in ways that are highly consistent with the assumptions adopted by the PDP framework.}

\keywords{distributed representations, PDP, MVPA, SOSLasso, fMRI, cognitive neuroscience}

%%% BEGIN DOCUMENT
\begin{document}

\maketitle
% \tableofcontents

% INTRODUCTION
An important question for cognitive science concerns the nature of mental representations. What are they, how are they structured, where do they come from, and how do cognitive processes operate on them to support behavior? One approach to cognition, variously referred to as the parallel distributed processing (PDP), connectionist, or neural network approach, has offered fairly specific answers to these questions. It begins with the central tenet of cognitive neuroscience that all cognitive processes ultimately arise from the propagation of activity amongst large populations of neurons communicating their states via excitatory and inhibitory synaptic connections; but it further proposes that the important characteristics of such systems can be captured in simplified abstract form by computer models that simulate the flow of information through networks of neuron-like processing units connected by weighted synapse-like connections. All cognitive abilities are proposed to arise from representational and processing mechanisms that can be so described and understood. Accordingly, the framework offers a constrained view of what mental representations are: patterns of neural activity evoked by a stimulus or a process over a set of units operating in parallel. Often such representations are proposed to be highly distributed, so that (1) any given unit contributes to the representation of many different items, (2) any given item representation is encoded over many different units and (3) the representation inheres in the full pattern of activation over all units, and not in the activation of individual units.

The case for distributed representations runs much deeper than being an implementational detail within a certain modeling formalism. Indeed, distributed representations of this kind have several properties that make them useful for understanding various aspects of cognition. For instance, they provide a natural basis for similarity-based generalization \cite{greer_emergence_2001}. Two different items that generate overlapping patterns of activation over the same units will tend to produce similar responses downstream in the network. Thus distributed representations explain how prior learning supports the processing of novel inputs, an ability central to accounts of categorization, inductive inference, language processing, and many other cognitive phenomena. Similarity-based generalization also naturally produces patterns of behavior observed in many different tasks, such as typicality effects, frequency effects, and effects of quasi-regularity \cite{plaut_understanding_1996-1, rogers_object_2003}. Distributed representations explain why, with neuropathology, cognitive abilities are not disrupted in an all-or-none fashion, but instead degrade gracefully: when some units in the representation are destroyed or disrupted, the remaining units continue to communicate with downstream units, transmitting information that may still be at least approximately correct \cite{devlin_category-specific_1998,rogers_structure_2004}. They provide a means of understanding the acquisition of new representations: rather than adding new information to a database, or new representational elements into a processing system, new representations can be acquired by adjusting connections within the network so that a given stimulus or process generates a new pattern of activity over the existing units. Finally, distributed representations can be highly efficient. With a local code, in which each unit represents one and only one item of information, then with n units it is possible to represent just n items. With a binary distributed coded, the same n units can represent 2n distinct items; and if the units encode continuous activation values there is, in principle, no limit to the number of items a given set of units can encode \cite{hinton_distributed_1984}.

These virtues are well known and form the basis for influential accounts of healthy, disordered, and developing cognition in theories of reading \cite{patterson_connections_1989, seidenberg_distributed_1989}, inflectional morphology \cite{bird_deficits_2003, patterson_deficits_2001, plunkett_connectionist_1999}, semantic processing \cite{hoffman_semantic_2011,lambon_ralph_neural_2007,rogers_semantic_2004}, and many other domains. Perhaps surprisingly, however, remarkably little work in cognitive neuroscience has attempted to directly test PDP's assumptions about the nature of mental representations. The reasons for this gap are probably methodological, at least in part. The most ubiquitous methods in human cognitive neuroscience---fMRI and other functional brain imaging technologies---typically yield vast amounts of noisy data.  To discern interesting patterns in these datasets, or to test particular hypotheses, the statistical models employed must adopt specific assumptions about the nature of the underlying signal. For many years, standard statistical approaches were built upon representational assumptions that were at odds with those adopted under the PDP approach. As a result it has been difficult to relate the results of such analyses to the predictions of PDP models. This has begun to change with the advent of new multivariate methods for analyzing brain imaging data but many challenges still remain. Several different approaches have appeared in the literature in recent years, each carrying with it particular assumptions about the nature of the underlying neural code, and so bringing biases in the kind of signal it is capable of detecting. Moreover, the nature of the assumptions and their implications may not always be completely transparent. Thus, although the PDP framework (and related approaches) put forward quite specific hypotheses about the nature of representations in the mind and brain, it has not been clear how neuroscience methods, and in particular functional brain imaging, might best be leveraged to test these hypotheses.

The goal of the current paper is to consider how the analysis of brain imaging data might best be approached if the PDP assumptions about mental representation are valid. We first consider in more detail what the PDP assumptions are and why they pose challenges for standard and even many state of the art brain imaging methods. To illustrate these points, we compare and contrast the results yielded by five different statistical methods in the analysis of the activation patterns generated by different inputs to a simple PDP model. We take the activation of a single unit as a model analog of the mean activity in a population of neighboring neurons, similar to that estimated from changes in the BOLD response at a single voxel. Since the behavior, architecture, and representational structure embedded in the simple model are fully known, it is possible to measure the extent to which the various methods succeed in identifying the model components that encode interesting representational structure. This analysis thus illustrates the strengths and weaknesses of different approaches if PDP assumptions about representation are valid. The results suggest a new strategy for the analysis of functional imaging data that may help to better connect PDP models to cognitive neuroscience. We then assess the utility of this approach by comparing its results to those of other state of the art methods in the analysis of one well-known publicly released fMRI dataset.

\section{A brief overview of PDP models and the representational assumptions they adopt}
PDP models are composed of simple neuron-like processing units that communicate via weighted synapse-like connections. Each unit adopts an activation state, typically varying between 0 and 1, that can be viewed as expressing the mean firing rate of a population of spiking neurons proportional to their maximal rate. Units transmit information about their current activation through weighted connections, which can be viewed as capturing the net effect of activity in one population of neurons on another. Weights are typically real-valued, with negative numbers indicating a net inhibitory effect and positive numbers indicating a net excitatory effect. Each unit computes a simple process: it adjusts its current activation state according to the input it receives from other units in the networks. If a given receiving unit receives inputs from a set of n sending units, then the input is usually computed as the inner product of the activation across all sending units and the values of the weights projecting from the sending units to the receiving units. The unit then converts the net input into a new activation state according to a specified transfer function (often a sigmoidal function of the net input). All units are conceived as computing inputs and updating activation states in parallel (hence ``parallel'' distributed processing), though on serial computers this parallel process is simulated by updating units in randomly permuted order.

Within a network, units are generally organized into layers, which govern the overall connectivity of the network: units within a layer tend to receive connections from, and direct connections toward, a similar set of units elsewhere in the network. Typically a subset of the units are specified to receive inputs directly from sensory systems (or other input systems outside the model), and to direct outputs toward motor systems (or other output systems outside the model). These unit subsets encode the input provided to the model and the outputs that simulate the model response. They are often referred to as visible units, because the theorist directly stipulates how different stimulus events and behaviors are represented with patterns of activation over the input and output units. Most models also include sets of units whose inputs and outputs are directed only to other units contained within the model—they do not receive external inputs from or direct outputs toward the model environment. For these hidden units, the theorist does not stipulate how different stimulus events or behaviors are to be coded with patterns of activation. Instead, the patterns of activation that arise across these units are determined solely by the values of the interconnecting weights. 

The weights themselves are viewed as being shaped by learning and experience. Many different learning algorithms have been explored in this framework, but all share the general idea that the weights gradually change over time in order to optimize some objective function---for instance, minimizing the discrepancy between the outputs the model generates and the correct ``target'' outputs---as the network processes information from different stimulus events. Because the weights adapt to experience, and because the patterns of activation over hidden units depend upon the weight values, PDP models are therefore capable of acquiring learned internal representations: the patterns of activation generated over hidden units by a given stimulus after the network has undergone learning in a model environment.  One interesting aspect of PDP models, responsible for their utility in many different cognitive domains, concerns the nature of the internal representations they acquire after learning in a structured environment. Often the models can acquire internal representations that may seem counter-intuitive from other points of view, but that can be shown, through computer simulations, to be capable of supporting the interesting behaviors documented in the domain of interest. Figure 1 and its caption provide one example of a PDP model used to understand aspects of semantic memory.

\textbf{---Figure 1 about here---}

With this overview of how PDP models work, we are ready to consider the challenges that the PDP framework raises for the discovery of mental representations in functional brain imaging data. Many difficult issues arise, of course, in any effort to relate artificial neural networks to real neural networks. Because network models are functional abstractions of the neural processes they aim to uncover, they necessarily gloss the complexity, and many aspects of structure and behavior, known to be important in real nervous systems. Acknowledging the interest, import, and complexity of these issues, we here adopt a fairly simplified stance on the relationship between network models and the brain networks we seek to discover in imaging data. Specifically, we assume that the activation of a single unit in a network model is analogous to mean neural activity in a population of hundreds of neighboring neurons within a small volume of cortex as estimated at a single voxel from BOLD activity in fMRI. Thus we will treat the pattern of activation generated by a given stimulus over units in a model network as analogous to the set of beta coefficients estimated over voxels from the BOLD response evoked by a given stimulus in a sparse event-related design. 

Even with this relatively transparent view of the relation between model elements and measured physiological responses, PDP raises four difficult challenges for the discovery of representational structure in the brain.

\begin{APAenumerate}
\item The behavior of a given cortical subregion (i.e., voxel or voxel cluster) can vary substantially across individuals even if different individuals encode the same representational structure across the same general regions.

For any given network, there are typically many different weight configurations that can generate appropriate outputs given the various inputs. The particular configuration that a network discovers with learning can depend on many things, including the initial random weight configuration, the ordering and distribution of the learning experiences sampled from the environment, and the effects of noise in the unit activations and/or weight changes. Thus a particular hidden unit in a given model can, across different training runs in the exact same environment, exhibit quite different patterns of activation in response to a given input. Yet the internal representations learned by a network are not arbitrary; the learning models are of interest because they reliably extract important kinds of structure across the set of input and output patterns to which they are exposed. What varies is the particular way that individual units contribute to encoding the interesting structure across network runs.

It is worth noting that this consequence of distributed representations may pose greater problems for finding signal in some cortical regions than others. In peripheral regions (i.e., early sensory and motor cortices), it is clear that information is encoded in largely the same way, and with a largely similar neuroanatomical organization, across individuals. In association cortices, it may be that neural codes are more strongly shaped by learning and experience, so that the way information is organized across cortex is more highly variable. PDP models provide a rough analog to this state of affairs, insofar as input and output units for a given model are stipulated to represent information in the same way in every model training run---that is, in every model "individual." The issues of variability in representation mainly apply to learned internal representations coded across hidden units.

\item Activation of individual units may not be interpretable independent of other units.

A corollary of the preceding points is that the behavior of a given cortical unit may not be interpretable, or may have quite different interpretations across individuals, when analyzed independently from other units. 
%A full consideration of the conjunctive qualities of distributed representations is beyond that scope of this article, but is explained well by \citeA{rogers_precis_2008}. Suffice it to say, that t
This property of distributed representation is important because it suggests that univariate approaches to data analysis--methods that assess the behavior of individual voxels or voxel clusters independently--can fail to uncover important components of neural representations. Wherever the interesting structure is embedded in activations across multiple cortical units, but is not reflected in individual units, such methods will yield null results.

\item The functional model architecture may not map transparently onto anatomical structure in the brain.

 A third issue concerns the relationship between the functional architecture of a computational model used to simulate performance on a task of interest and the actual anatomical structure of the corresponding neural network in the brain.  As noted earlier, units in PDP models are organized into layers, with units in a given layer receiving connections from and directing connections toward the same subsets of units elsewhere in a network. The layer is a useful construct for understanding how a network functions, insofar as the units within a layer, by virtue of having similar connectivity to the broader network, ``work together'' to represent and process the same information. Distributed internal representations in PDP networks are typically viewed, therefore, as being encoded across units within a particular layer.  

It may seem natural to view layers as model analogs of cortical regions, so that the gross architecture of a computational model maps transparently onto the anatomical structure of networks in the brain that carry out the modeled cognitive function. Though this analogy is reasonable, it is not the only possible way that the functional architecture of a computational model might relate to the neuroanatomical structure of a corresponding cortical network. In fact, the layers of a computational model do not, in principle, have any implications for how the corresponding cortical units might be anatomically situated in the brain. Units that function together as a ``layer'' could be situated in multiple different cortical regions, or widely dispersed anatomically, or interdigitated with other units subserving different functions. The defining property of layers in a computational model is their pattern of connectivity in the gross architecture, and the same network connectivity can exist among many different spatial arrangements of units [FIGURE?]. In other words, the relationship between the functional architecture of a computational model---the grouping of units into layers as typically depicted in model figures, for instance---may not transparently reflect the topological arrangement of the corresponding cortical units in the brain. 

This lack of transparency poses a problem for approaches to brain imaging that assume representations to be encoded over a volume of anatomically contiguous cortical units, including approaches that average signal over regions of interest, that spatially blur signal, or that restrict statistical analysis only to voxels within pre-specified areas. If cortical units that function together as a representational substrate do not happen to reside in a single contiguous cortical region, such methods may fail to discover important signal.

\item The network of interest in any given study co-exists in the brain with many other networks, all subserving other functions that may not be of interest.

Any given computational model is designed to aid understanding of a particular aspect of cognition, and typically includes only those elements that the theory stipulates to be important for the behavior of interest. Even if the model is a relatively faithful and accurate abstraction of a real cortical network in the brain, the physiological measurements generated by that network will, in any brain imaging study, be intermingled with measurements from a great many other cortical systems involved in other aspects of cognition unrelated to the task of interest. Since the tasks we study as cognitive neuroscientists tend to be comparatively simple, odds are that the great majority of measurements taken will reflect metabolic activity unrelated to the representational structure we are searching for. Thus the effort to find distributed representations in brain imaging data raises thoughts of needles and haystacks. If we cannot narrow the search by only considering contiguous volumes of units, or pre-selecting regions of interest, or reducing the complexity of the data by spatial smoothing, it is not clear how the needles are to be found.

\end{APAenumerate}

\subsection{Summary}
The representational assumptions of the PDP framework lead to the following somewhat bleak-seeming view. The behaviors of individual cortical units (read voxels) may not independently covary with or otherwise reflect the objects of representation we are interested in finding in a systematic way across individuals. Mental representations may instead inhere in the patterns of activation evoked across whole sets of units that together function as a representational ensemble by virtue of their connectivity within the overall cortical network (like the layers of a neural network model). This is the core sense in which representations are distributed in the PDP view. The way that a particular unit contributes to different representations can be highly variable across individuals, even if the ensemble encodes the same representational structure across individuals. This means that the search for voxels exhibiting similar responses in similar anatomical locations across people will fail to reveal important representational structure. Moreover, the units that operate together as a representational ensemble may not be anatomical neighbors, and are certain to be buried within the mountain of measurements provided by functional imaging technologies across the whole brain. These possibilities raise a daunting challenge: representational structure can only be discerned by considering the whole pattern of activation over a representational ensemble; yet the units within such an ensemble may be anatomically dispersed and intermingled with a vast amount of irrelevant information. One cannot understand the representation without knowing which units together constitute an ensemble, but how is one to find the ensemble in the first place?

In what follows, we assess how well different approaches to fMRI data analysis meet this challenge by applying them to the discovery of representational structure in data generated by a simple neural network model as it processes different input patterns. We will see that all methods bring with them important biases in the kinds of representational structure they are capable of detecting, and that some methods are better-suited to finding the distributed structure that the PDP framework assumes. We will also see that patterns of results across methods can provide important information about the nature of the representations encoded in different parts of a network.

\section{Model and Methods}

\textbf{---Figure 2 about here---}

The model we will employ for this analysis is illustrated in Figure 2. It is an auto-encoder network: when presented with an experience in the form of a pattern of activity over its 36 input units, it learns to reproduce that same pattern over its 36 output units. Auto-encoder networks have been used as simple models of human memory, because once they have learned they are capable of both retrieving full information from a partial cue and of generalizing prior learning to new items. In this case, however, we do not intend the model to embody a specific hypothesis about a particular real-world cognitive function. Instead, the model is designed to make explicit the challenges noted in the introduction. 

To this end, the patterns that the model processes are viewed as coming from two different \emph{domains}, A and B, corresponding to some cognitive distinction of theoretical import. For instance, A and B might correspond to nouns versus verbs, or animals versus manmade objects, or faces versus non-faces, or any other binary distinction thought to be of potential relevance to behavior.  Each individual item is represented with a unique pattern of activation over input units, and the network’s task is simply to generate the same pattern output units. In this sense, there is no explicit representation of the two classes A and B in the inputs, outputs, or network task. The two domains are assumed, however, to be distinguishable from the distribution of input/output properties they possess. Specifically, one subset of input/output properties is marginally more likely to be active for items from domain A, while another subset is marginally more likely to be active for items in domain B. We will refer to these subsets together as \emph{systematic I/O} units, because they each weakly covary with the representational distinction of interest.  Each item also possesses many \emph{arbitrary I/O} units that do not systematically differ between domains.

After the model has learned, it is possible to ``query'' it by presenting an input pattern and generating patterns of activation throughout the rest of the network. As noted earlier, we take the activation at each unit in response to an input as a model analog of the neural response to a stimulus estimated from the BOLD signal at a single voxel in a single individual. Across different training runs, the model will always exhibit the same overt behavior (generating the correct pattern over output units), but arising from different configurations of weights, and hence from different internal representations. Variability in weight configurations and internal representations acquired across different training runs thus provides a model analog of individual variability in the neural representations acquired across the population. To simulate data generated by a functional brain imaging study with, say, 10 participants, we train the model 10 times, and for each trained model, record the pattern of activation generated over all model units by each input pattern (i.e., stimulus).  The question we then wish to ask, by applying different statistical methods to the analysis of this synthetic imaging data from a sample of trained models, is the following: where are representations of the domains A and B encoded in the network?

The network architecture is designed so that there are two possible answers to this question. The first answer is that representations of A and B are directly encoded in the activations of systematic I/O units. For all input and output units, the response of a given unit to a particular item is directly specified by the environment, so that these units will always respond to a given stimulus in the same way across training runs. Each systematic I/O unit has a marginally different probability of being active depending upon the domain; in this sense the A units each independently encode a representation of the A domain and the B units encode a representation of the B domain. The relationship between domain and activation is, however, stipulated to be quite loose: for any A item, many A units will be inactive and a few B units may be active (and vice versa). Thus different items in the A domain have quite different representations over A units, and the correlation between activation and domain is weak for any individual unit. Nevertheless, each input and output units responds to stimuli in exactly the same way across model individuals (subject to noise in measurement as explained below). We further stipulate that the A input and output units are anatomical neighbors, as are the B input and output units, and that this anatomical arrangement is exactly the same across individuals. Thus the systematic I/O units individually encode a \emph{weak} distinction between A and B that is \emph{consistent} across model individuals and is \emph{anatomically localized} within input and output layers. 

The second answer is that the representations of A and B domains are encoded in a distributed fashion over a subset of model hidden units. As shown in the Figure, the input units project to the output units by way of two separate hidden layers. The \emph{systematic hidden layer} (SH) contains 7 hidden units that receive connections from the systematic input units and send connections to the systematic output units. The \emph{arbitrary hidden layer} (AH) contains 29 units that receive connections from the arbitrary inputs, and send connections to \emph{both} the systematic and arbitrary outputs.  The weights are shaped by learning, so each input generates a pattern of activation over both the SH and AH layers, each corresponding to a learned internal representation of the stimulus. The particular way that layers are connected, however, ensures that these internal representations will have specific representational properties. The SH layer contains relatively few units, making it difficult for this pathway to represent many arbitrary distinctions amongst the various items. Also, it only passes activation from systematic input to systematic output units, and it is across these systematic I/O units that the domain structure is (weakly) expressed. For these reasons, the SH layer acquires distributed internal representations that reflect this structure:  items within a domain are represented with quite similar patterns while items from different domains are represented with quite distinct patterns. In contrast, the AH layer has many units, so it can more easily express the many arbitrary distinctions amongst items in the environment. It also receives inputs only from the arbitrary input units. Consequently, the AH layer acquires distributed internal representations that have very little structure; and the weights in the arbitrary pathways serve to ``memorize'' both the arbitrary features and the idiosyncratic ways that each item differs from its neighbors in systematic properties. In other words, the architecture produces a division of labor, such that the systematic hidden layer is well-suited to acquiring distributed representations that reflect the domain structure (representing items between domains as dissimilar and items within domains as similar), while the arbitrary layer acquires knowledge about idiosyncratic distinctions among items. Thus the systematic hidden units, taken together, are very diagnostic of each item's domain. If what matters for the discovery of a representation is the fidelity of the distributed code---the precision with which is expresses the category structure motivating the contrast of interest---then a good method should identify the small number SH units as important for representation.

\textbf{---Figure 3 about here---}

With this general understanding of the model behavior, let's consider how it makes explicit the four challenges for brain imaging noted earlier. First, although the same representational structure is observed across all network runs, the particular way it is expressed across individual units is essentially arbitrary for the reasons noted in the introduction (challenge 1). Second, the mean activations of SH units taken independently do not systematically differ for items in the A and B domains: to find the important structure, one must consider the pattern evoked over multiple units (challenge 2). Third, the functional architecture of the model shown in Figure 2 can be anatomically arranged in many different ways (challenge 3). To make this issue explicit, we consider two different topographic arrangements of the functional model. In the first, units within the same layer are always situated as anatomical neighbors, so that the representations encoded by the SH and the AH layers are \emph{anatomically localized}. In the second arrangement, we assume that the SH units are \emph{spatially intermingled} with the AH units, in a different way across model individuals, so that the representations they encode are \emph{anatomically dispersed}. In the results we will consider how well each method identifies the SH units as a function of whether they are localized or dispersed. Finally, the model captures the idea that the units of interest constitute only a small proportion of all the units measured (challenge 4). In the model itself, most of the units are dedicated to processing arbitrary patterns (18 arbitrary I/O units and 29 arbitrary hidden units). The next largest set of units are the systematic I/O units that weakly but consistently encode the category structure. The units of greatest interest, the SH units, constitute just 6\% percent of all the units in the model. To make the problem even more dire, we assume that all units in the model are embedded with other measured units whose responses vary randomly with the stimulus (see Methods). 

\subsection{Summary} 

Though very simple, this auto-encoder network captures each of the challenges noted in the introduction: it acquires distributed internal representations that express representational structure of interest; the way the structure is coded across units varies in different individual models; the structure cannot be discerned from the activations of single units but arises in patterns over multiple units; the relationship between the functional architecture and the underlying model topography can be opaque; and the units that encode the structure we wish to discover are buried in a large number of other measurements. The question we now address is how well different analysis methods fare at discovering representational structure across both systematic I/O units and the SH units, when they are applied to data generated from a sample of model training runs.

\section{Simulation details}
The model outlined above has 36 input units (18 systematic), 36 hidden units (7 systematic), and 36 output units (18 systematic). The model's environment consists of 72 input patterns, which are sampled from two domains, distinguishable based on their systematic input. Half of the input patterns activate units from the first 9 systematic input units (domain A items) and the other half activate units from the second 9 systematic input units (domain B items). The construction of these input patterns was carefully balanced, so that every pattern consisted of exactly 2 systematic and 2 arbitrary input units, that every unit was active in exactly 8 input patterns. Arbitrary units were balanced as closely as possible with respect to category. This all ensures that systematic input units are equally weakly informative about what category a thing belongs to, and that arbitrary units are devoid of category information.  

The model was fit in LENS \cite{rohde_lens:_1999} using back-propogation to minimize cross-entropy. The weights were adjusted with momentum and subject to weight decay which decreased the size of all weights by 0.1\% after every epoch. The model was trained 10 times to asymptotic performance with very low error over 1000 epochs. These 10 models were used to generate data for 10 ``subjects'', based on the patterns of activity over the whole network in the presence of each item. Despite being trained on exactly the same 72 items and obtaining the same level of performance, each model learned different internal representations over their hidden units. Each subject's data set is a 1-dimensional vector of values, arranged from the first input unit to the last output unit. Prior to any reordering, the data for each subject contains well localized signal that is consistently located across subjects.

These 10 datasets contain the ``true'' response pattern for each subject to each item. In any real application, true signal is buried in noise. Of course, the properties of this noise and the signal to noise ratio are important considerations in reality; they will be handled very simply in these simulations. All true activation ranges from 0 to 1, and we added a i.i.d Gaussian random value to each unit, N(0,1). 

\section{Results}

\section{Discussion}
% KEY CITATIONS
% - Jarrod and Brad, 2012
% - The MVPA 2008 winners?
% - Large-scale brain networks in cognition (Bressler & Menon, 2010)


\bibliography{MASTERBIB/zotero}

\end{document}

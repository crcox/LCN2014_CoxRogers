\documentclass[12pt,man]{apa6}
\usepackage[nodoi]{apacite}
\usepackage{boolexpr}
\usepackage{epstopdf}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{array}

\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\DeclareDelayedFloatFlavor{sidewaystable}{table}
\DeclareDelayedFloatFlavor{sidewaysfigure}{figure}

\input{tables/table_assumptions_content.tex}
%\input{tables/table_assumptions_caption.tex}
\input{tables/table_results_content.tex}
%\input{tables/table_results_caption.tex}
\DeclareMathOperator*{\argmin}{arg\,min}

% % % IMPORTANT % % %
% This will define the spelling of "SOS LASSO"
\newcommand{\soslasso}{SOS LASSO } % trailing space required
% This will define the formatting and punctuation of "MATLAB"
\newcommand{\matlab}{MATLAB\textregistered }

\title{Taking distributed representations seriously in functional brain imaging}
\shorttitle{Distributed representations}
\author{Christopher R. Cox\\Timothy T. Rogers}
\affiliation{University of Wisconsin, Madison}

\input{abstract.tex}

\authornote{We would like to acknowledge Nikhil Rao and Robert Nowak, PhD, for developing the SOS LASSO loss function and its \matlab  implementation, and for continued collaboration and discussion regarding the application of machine learning to cognitive neuroscience.\\
We have no conflicts of interest with respect to their authorship or the publication of this article.
} 

\note{ %
	\texttt{crcox@wisc.edu}\footnote{Corresponding author.} \\ \texttt{ttrogers@wisc.edu} \\
	W J Brogden Psychology Building \\
	1202 West Johnson Street, rm 238 \\
	Madison, WI 53706 \\
}

\keywords{distributed representations, PDP, MVPA, \soslasso, fMRI}

%%% BEGIN DOCUMENT
\begin{document}

\maketitle
% \tableofcontents

% INTRODUCTION
\input{introduction.tex}

\input{model_and_methods.tex}

\input{simulation_details.tex}

\section{Results}

With this understanding of the model, we are now ready to consider how different statistical methods for fMRI fare at discovering the model units that encode representations of the two domains, both in the case where the hidden units are anatomically localized and when they are anatomically dispersed. The methods we consider include the standard univariate contrast method and four forms of multivariate pattern classification (MVPC). Each method faces the challenges inherent in fMRI analysis---that of finding meaningful signal within a vast amount of quite noisy data. To address the challenge, each method adopts a different set of assumptions about the nature of the underlying signal, and so brings with it biases in the kinds of results it yields. For each method, we will begin with a brief exposition of the basic logic and essential concepts and will explicitly note the underlying representational assumptions. We then report the implementational details and results of the analysis, with the aim of answering four questions:

\begin{enumerate}
\item Does the method identify the systematic I/O units, but not arbitrary units, as important for domain representation?
\item Does the method identify the systematic hidden units, but not arbitrary units, as important for the domain representation?
\item Do the results differ when hidden units are anatomically localized versus dispersed?
\item Does the method indicate differences in how the information of interest is coded across unit sets? Specifically, does it indicate that some units respond more to A items than to B items, others show the reverse pattern, and still others express the A/B distinction with a distributed code? 
\end{enumerate}

\input{univariate.tex}

\input{searchlight.tex}

\input{regularized_regression.tex}

\input{soslasso.tex}

\input{summary_of_model_results.tex}

\input{cmu_results.tex}

\input{discussion.tex}

%\bibliographystyle{/home/chris/texmf/tex/latex/apacite/apacite}
\bibliographystyle{apacite}
\bibliography{MASTERBIB2/AuthorIDs,MASTERBIB2/Main,MASTERBIB2/ttr}

\end{document}


%\textbf{---Figure 6 about here---}
%\begin{figure}
%\centering
%\includegraphics[width=0.75\textwidth]{figures/regression.eps}
%\caption{\label{fig.regression} The results from the three regularized regression analyses. Because the model weights themselves are biased estimates and on an arbitrary scale, the height of each bar corresponds to the number of number of times each unit was selected over subjects. Colored bars were selected more times than would be expected given the overall rate of unit selection. The blueness or redness of the bar conveys the proportion of the time each unit was assigned a positive weight over subjects. Positive weights mean that activation at that unit will push the model towards labeling the current item as belonging to domain A.}
%\end{figure}
%
%\textbf{---Figure 7 about here---}
%\begin{figure}
%\centering
%\includegraphics[width=0.5\textwidth]{figures/cmu_searchlight.eps}
%\caption{\label{fig.cmu_searchlight} Information map yielded from applying searchlight to the Mitchell et al (2003) ``star-plus'' dataset, for one slice of the brain. The redness at each point indexes the number of times a searchlight centered on that voxel yielded above chance classification over subjects.}
%\end{figure}
%
%\textbf{---Figure 8 about here---}
%\begin{figure}
%\centering
%\includegraphics[width=.9\textwidth]{figures/cmu_regression.eps}
%\caption{\label{fig.cmu_regression}  The information maps yielded from applying the three regularized regression analyses to the same ``star-plus'' dataset. Because the solutions are very sparse for LASSO and \soslasso, all voxels selected for any subject are plotted. The blueness or redness of the dots conveys the proportion of the time each unit was assigned a positive weight over subjects. Positive weights mean that activation at that unit will push the model towards labeling the current item as belonging to domain A.}
%\end{figure}




%While the application to the star-plus data is an interesting proof of concept, many aspects of the data and the experiment from which the data were derived preclude many important questions from being addressed.  

%Riggall and Postle's (2012) use of multiple methods to interrogate the data and draw conclusions about the nature of  neural representations  leads us to a closing remark about which method is ``best'' or ``right''. Although some methods seemed to ``out perform'' others on our simulated datasets, we are emphatically not making a claim about the universal superiority of one method over another. All have their place; however, all have their biases and limitations. Using one at the  
\documentclass[12pt,man,draftfirst]{apa6}
\usepackage[nodoi]{apacite}
\usepackage{epstopdf}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% % % IMPORTANT % % %
% This will define the spelling of "SOS LASSO"
\newcommand{\soslasso}{SOS LASSO } % trailing space required
% This will define the formatting and punctuation of "MATLAB"
\newcommand{\matlab}{MATLAB\textregistered }

\title{Taking distributed representations seriously}
\shorttitle{Taking distributed representations seriously}
\author{Christopher R. Cox\\Timothy T. Rogers}
\affiliation{University of Wisconsin, Madison}

\abstract{The Parallel Distributed Processing (PDP) approach to cognition assumes that active mental representations are distributed over many neural populations. It is known that distributed representations can be acquired through domain general learning mechanisms to economically encode graded similarity structure that supports generalization and is robust to disruption. Until recently, however, the PDP hypothesis has not strongly influenced functional  brain imaging, which has, for historical and methodological reasons, tended to adopt modular 
assumptions about how the brain encodes information. Multivoxel pattern analysis (MVPA) relaxes these assumptions, but the most common methods are only sensitive to quite localized similarity structure, either within a searchlight or predetermined regions of interest. In this paper we leverage a recent innovation in multivariate analysis, the ``sparse overlapping sets Lasso'' (\soslasso), to seriously consider whether distributed representations like those that arise in PDP models are also employed in the brain.By applying \soslasso and univariate methods to data generated by artificial neural networks where the representational structure is known, we show how and why reliable univariate results can systematically miss important distributed patterns where \soslasso preferentially identifies such patterns. We then apply both methods to real brain imaging data, and show that, at least in some domains of interest, the underlying representations appear to be distributed in ways that are highly consistent with the assumptions adopted by the PDP framework.}

\keywords{distributed representations, PDP, MVPA, \soslasso, fMRI, cognitive neuroscience}

%%% BEGIN DOCUMENT
\begin{document}

\maketitle
% \tableofcontents

% INTRODUCTION
\input{introduction.tex}

\input{model_and_methods.tex}

\input{simulation_details.tex}

\section{Results}

With this understanding of the model, we are now ready to consider how different statistical methods for fMRI fare at discovering the model units that encode representations of the two domains, both in the case where the hidden units are anatomically localized and when they are anatomically dispersed. The methods we consider include the standard univariate contrast method and four forms of multivariate pattern classification (MVPC). Each method faces the challenges inherent in fMRI analysis---that of finding meaningful signal within a vast amount of quite noisy data. To address the challenge, each method adopts a different set of assumptions about the nature of the underlying signal, and so brings with it biases in the kinds of results it yields. For each method, we will begin with a brief exposition of the basic logic and essential concepts and will explicitly note the underlying representational assumptions. We then report the implementational details and results of the analysis, with the aim of answering four questions:

\begin{enumerate}
\item Does the method identify the systematic I/O units, but not arbitrary units, as important for domain representation?
\item Does the method identify the systematic hidden units, but not arbitrary units, as important for the domain representation?
\item Do the results differ when hidden units are anatomically localized versus dispersed?
\item Does the method indicate differences in how the information of interest is coded across unit sets? Specifically, does it indicate that some units respond more to A items than to B items, others show the reverse pattern, and still others express the A/B distinction with a distributed code? 
\end{enumerate}

\input{univariate.tex}

\input{searchlight.tex}

\input{regularized_regression.tex}

\input{soslasso.tex}

\input{summary_of_model_results.tex}

\input{cmu_results.tex}

\input{discussion.tex}

\bibliographystyle{/home/chris/texmf/tex/latex/apacite/apacite}
\bibliography{MASTERBIB2/zotero}

\end{document}


%\textbf{---Figure 6 about here---}
%\begin{figure}
%\centering
%\includegraphics[width=0.75\textwidth]{figures/regression.eps}
%\caption{\label{fig.regression} The results from the three regularized regression analyses. Because the model weights themselves are biased estimates and on an arbitrary scale, the height of each bar corresponds to the number of number of times each unit was selected over subjects. Colored bars were selected more times than would be expected given the overall rate of unit selection. The blueness or redness of the bar conveys the proportion of the time each unit was assigned a positive weight over subjects. Positive weights mean that activation at that unit will push the model towards labeling the current item as belonging to domain A.}
%\end{figure}
%
%\textbf{---Figure 7 about here---}
%\begin{figure}
%\centering
%\includegraphics[width=0.5\textwidth]{figures/cmu_searchlight.eps}
%\caption{\label{fig.cmu_searchlight} Information map yielded from applying searchlight to the Mitchell et al (2003) ``star-plus'' dataset, for one slice of the brain. The redness at each point indexes the number of times a searchlight centered on that voxel yielded above chance classification over subjects.}
%\end{figure}
%
%\textbf{---Figure 8 about here---}
%\begin{figure}
%\centering
%\includegraphics[width=.9\textwidth]{figures/cmu_regression.eps}
%\caption{\label{fig.cmu_regression}  The information maps yielded from applying the three regularized regression analyses to the same ``star-plus'' dataset. Because the solutions are very sparse for LASSO and \soslasso, all voxels selected for any subject are plotted. The blueness or redness of the dots conveys the proportion of the time each unit was assigned a positive weight over subjects. Positive weights mean that activation at that unit will push the model towards labeling the current item as belonging to domain A.}
%\end{figure}




%While the application to the star-plus data is an interesting proof of concept, many aspects of the data and the experiment from which the data were derived preclude many important questions from being addressed.  

%Riggall and Postle's (2012) use of multiple methods to interrogate the data and draw conclusions about the nature of  neural representations  leads us to a closing remark about which method is ``best'' or ``right''. Although some methods seemed to ``out perform'' others on our simulated datasets, we are emphatically not making a claim about the universal superiority of one method over another. All have their place; however, all have their biases and limitations. Using one at the  
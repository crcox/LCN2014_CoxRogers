\section{Application to real data}

Before turning to a broader consideration of the implications of our results, it is useful to consider whether the methods of interest yield appreciably different results when applied to real fMRI data. If they don't, most of the discussion to this point is essentially moot. For this reason, we compared the results of the four different multivariate methods when applied to the publicly-released star-plus dataset \cite{starplus}. In each trial of this study, participants viewed a visual display showing simple shapes in a particular spatial configuration, and also read a sentence describing shapes in a particular spatial arrangement (e.g. "The star is above the triangle"). The task was to decide whether the verbal statement was true of the visual display. The two stimulus types (sentence versus display) were counterbalanced for order. Participants performed the task for X minutes while their brains were scanned with fMRI, producing one measurement from each voxel every 2 seconds for a total of XX measurements. Measurements taken between trials were discarded, and remaining measurements were labeled as arising either from the sentence-processing half of a trial or from the visual-display half of the trial. The goal of the study was to learn classifier that could successfully decode, from the BOLD response taken at a given time, whether the subject was processing a sentence or a picture. To that end, the authors grouped all voxels for each subject into X anatomically-defined regions. Of these, they selected 7 regions hypothesized a-priori to encode information relevant to the classification task. BOLD signal was averaged over all voxels within a region, producing 7 mean BOLD values for each subject at each time point. The authors then trained a support-vector machine classifier on data from 5 subjects, effectively treating all observations as having come from a single participant. This approach led to cross-validation accuracy of XX for hold-out sets from the 5 participants. More interestingly, the classifier also showed above-chance classification of patterns measured in a 6th subject. Thus the classifier learned from a group of individuals generalized to a new individual, indicating some consistency in how and where the useful information is encoded across individuals.

The star-plus dataset is limited insofar as it contains functional data from only 6 participants, without accompanying anatomicals. Thus the data can be aligned only very roughly across subjects, and there is insufficient power to conduct cross-subject statistical tests. Importantly, however, the original work demonstrates that there exists cross-subject consistency in the data that can be exploited by multivariate methods. Thus we can be certain that there exist interesting relationships across subjects in where and how information relevant to the task is encoded.

Our aim in this analysis is to assess whether searchlight, LASSO, ridge regression, and SOS LASSO yield qualitatively similar or different results when applied to the same data. Note that, in contrast to the original work, none of these methods involves pre-selection of regions of interest or data reduction via averaging. Instead, each method is applied to the data in their raw form. To assess whether a method has discovered some useful representational structure, we first consider whether it shows better than chance cross-validation accuracy. The central questions then are (1) whether the methods identify similar or different voxel subsets, (2) whether they suggest similar or different conclusions about how information of interest is coded in voxel activations and (3) whether it is possible to adjudicate the validity of the different results.

\subsection{Implementation details}

XXX

\subsection{Results}

All methods exhibited significantly above-chance accuracy (p < XX in all cases), indicating that they all discover some reliable structure. Figure \ref{fig.brainweights} presents an aggregate image of the brain regions identified as encoding useful structure by the different methods. Identified voxels are aggreagated across the Z plane and projected on a single horizontal section taken through the middle of the temporal lobe, with anterior regions toward the top. 

For searchlight, voxel coloring indicates the proportion of participants for whom the surrounding searchlight led to reliable above-chance classification. Though the map is not statistically thresholded in the usual manner (owing to the previously-noted limitations of the dataset), the color intensity gives a sense of what such a map would look like. The signal appears to be highly distributed all throughout the measured regions, similar to the model results obtained when the searchlight was too large. There are no regions where signal is obviously absent. 

In the remaining panels, red indicates voxels where high BOLD was associated with picture processing, blue indicates voxels where high BOLD was associated with sentence processing, and purple indicates voxels with different interpretations across individuals. For LASSO, selected voxels are sparsely distributed throughout the measured regions, with no discernable spatial topography apart from a denser clustering of blue voxels toward the occipetal cortex. Most points are either red or blue because there was relatively little overlap in the selected voxels across subjects. The ridge regression plots show results at two levels of weight thresholding (10\% and 90\%). Here the voxel hue indicates the proportion of positive weights across individuals, while the intensity indicates the mean absolute value of the voxel weight across individuals. When few weights are discarded, the results are a "purple haze": almost everything is selected in some participant, and there is little apparent coherence across participants in how a given voxel encodes information. When most weights are discarded, XXX. Finally, the SOS LASSO plot identifies fewer voxels overall, but these tend to be loosely localized within three regions: the occipetal cortex, and left and right anterior temporal cortex. Within these regions, the direction of the classifier weights appears largely intermixed, consistent with the PDP hypothesis that the nature of the neural code can be highly variable both within and across individuals.

From these images, it is clear that the different methods do indeed yield very different conclusions about how the relevant signal is encoded. Which view are we to believe? Cross-validation accuracy of the different clasifiers might provide one clue: presumably solutions that have done a better job identifying important components of the representation should show better cross-validation performance. We therefore compared the mean cross-validation accuracy across the different methods. For searchlight, this was measured in each participant by taking the mean accuracy over all searchlights individually showing reliable above-chance classification. The other methods each directly yield a single measure of accuracy for each participant. Means and standard errors of these figures across participants are shown in Figure \ref{fig.brainacc}. SOS LASSO shows significantly better performance than the other three methods, suggesting that it has done a better job of picking out useful representational structure in each individual.

A second way of assessing the different solutions is to compare them to a-priori expectations. In this case, recall that the authors of the star-plus study originally identified 7 regions of interest where they expected functional activations to carry important information. Thus we can inquire how well the solution for each method aligns with these expectations, by computing what proportion of the identified voxels fall within these 7 ROIs. These data are shown in Figure \ref{fig.proportions}. The voxels identified by SOS LASSO are more likely to fall within the expert-defined ROIs than are the voxels identified by the other methods, providing a further validation of the SOS LASSO solution.



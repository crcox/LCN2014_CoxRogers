\section{General Discussion}

We began by noting that an important theoretical approach in cognitive science, the PDP approach, adopts specific claims about the nature of mental representations that, despite being quite useful for understanding many aspects of cognition, have been difficult to empirically test in functional brain imaging studies. PDP suggests that mental representations are patterns of activation distributed over neural populations, with information encoded, not in the activity of individual cortical units taken independently, but by the complete pattern coded over a representational ensemble. It further suggests that the response of any given element in an ensemble may vary arbitrarily across individuals, even if the ensemble jointly codes the same structure across individuals; that elements of an ensemble need not all be anatomical neighbors within an individual; and that their location across individuals, while not completely random, may be subject to at least some variability. These assumptions about representation, paired with the observation that functional brain imaging technologies yield up vast amounts of quite noisy data, present serious challenges to  discovery of the neural bases of mental representation.

We have now reviewed five different methods for analyzing brain imaging data, considered their underlying assumptions, and assessed their ability to discover distributed representations of the kind PDP assumes. From this analysis it is clear that very different results arise depending upon the statistical method employed or, in some cases, upon parameterization of the method in question. Each method succeeds best when the implicit assumptions it adopts are met in the data. Thus univariate contrast successfully identifies the systematic I/O units, which conform to the assumptions listed in the corresponding column of \ref{tab.assumptions}. The SH units, despite encoding cleaner domain representations, violate all of these assumptions and so are completely missed. Searchlight does well at detecting both systematic I/O and SH units, but only if the useful information is contained within the radius of a relatively small searchlight and is localized in the same way across individuals. {\lasso} performs modestly well at discovering SH units, but in assuming no consistency across model individuals and no redundancy within individuals, becomes highly susceptible to noise and so misses many important units. Ridge regression, in assuming highly redundant signal without care for overall sparsity, spreads weights over all units, making the solution hard to decipher even if the classifier performs well.

{\soslasso} succeeds best at finding distributed internal representations because it adopts assumptions that align well with PDP. Like other MVPA methods, no consistency in coding is assumed across individuals, allowing for differences in how the same representational structure is expressed over hidden units. In contrast to {\lasso} and ridge regression, representational units are assumed to be localized loosely within individuals, and with loose consistency of location across individuals, allowing for some degree of similarity in the global architecture. In contrast to univariate and searchlight methods, however, there is no assumption that all units within a given locale code the same representation, or that an informative region must contain sufficient information for a representation. This allows for the possibility that representations may be coded across multiple anatomically distal regions, and that the specific anatomical arrangement of the interesting elements can vary across individuals. The assumption that the features will be in roughly similar anatomical neighborhoods overcomes the limitations of {\lasso}, because the optimization will prefer units that may individually carry less direct information about the class label if these happen to reside near other informative units within or across individuals. Finally, the representation is assumed to be sparse overall, reflecting the notion that the representational elements of interest will typically be buried within a large system, with many measured components likely to be subserving unrelated functions. With these assumptions implemented in a whole-brain optimization, the method succeeds fairly well in finding the distributed representational structure that is central to cognition under the PDP view, but most difficult to find with other methods. 

Application of the same methods to the star-plus dataset further suggests that, in this case at least, neural representations have properties similar to those assumed by PDP. Specifically, the patterns of activation discovered by {\soslasso} were distributed over three different coarse anatomical regions, but with different subsets of units within each region selected for different individuals, and with different representational codes apparent across voxels within a given region. With different classifiers fit to each subject under these constraints, {\soslasso} performed significantly better than other methods at classifying hold-out images, suggesting that its assumptions led to discovery of more reliable signal within each individual. In line with this view, the features it selected were significantly more likely to lie within the regions of interest shown in the original work to carry important information about the distinction of interest.  

With these results, there are two broader questions we wish to consider.

{\em 1. Why bother with these complicated methods?} The skeptic might wonder about the point of these analyses. Functional brain imaging has proceeded just fine for three decades now based mainly on the localizationist univariate assumptions stated earlier. Many important findings have been so discovered, and indeed, many researchers may consider the central goal of cognitive neuroscience to be the discovery of which cognitive functions are associated with which brain regions. In fact, we suspect that this view is so firmly entrenched that some of the alternative possibilities considered earlier---that representations are not always anatomically localized within a region, or that the same regions may behave quite differently across individuals---might seem far-fetched to some. Put bluntly, the critic may assert that the PDP view of mental representations is just wrong. 

As a response to this point, we gesture toward the already large and rapidly growing literature on multivariate pattern analysis of functional brain imaging data, whose surface we have barely scratched. There exist many other approaches beyond those we have studied here, but by and large, they are all painting a quite different picture of neural representation and processing than suggested by the classical localizationist view. One observation is that, in many different tasks, information appears to be more widely distributed in the brain than previously thought. For instance, \citeA{mitchell_predicting_2008} trained a generative model to predict whole-brain responses to nouns when given a feature-based description of the noun meaning, and showed that, across the entire brain, words with more similar meanings evoked more similar overall patterns. \citeA{rish_sparse_2012} trained a variant of regularized regression similar to those explored here to predict the perceived magnitude of a stimulus from the neural response it evoked. On each iteration, she identified 100 voxels useful for this prediction task, then removed the voxels and retrained the classifier. Remarkably, she found above-chance prediction over many such iterations, retraining on an increasingly small set of predictors. The implication appears to be that, in this particular task, stimulus magnitude is reflected in the activation value of many different voxels throughout the brain. \citeA{kriegeskorte_representational_2008} has used unsupervised multidimensional scaling methods to show that the responses evoked by visual stimuli throughout ventral temporal cortex in both humans and non-human primates express coarse conceptual distinctions amongst object categories. These are just a few examples of a broad trend in the literature; many more were recently reviewed by \citeA{CoxSeidenbergRogersIP}.

A second observation is that the information contained in distributed patterns over voxels is not mirrored by the overall magnitude of activation observed within a given cortical region. That is, univariate contrast maps and information maps often paint very different pictures of where the interesting signal is, across several studies. This point was most famously made by \citeA{haxby_distributed_2001} in his MVPA analysis of the ventral temporal cortex, in which he showed that activation patterns in the putative fusiform face area (FFA) were sufficient to decode places from objects, while faces could be discriminated from objects and places from voxels outside the FFA. In a more recent example, \citeA{riggall_relationship_2012} conducted both univariate and multivariate (ridge regression) analyses of activation patterns elicited in a working memory task where participants ``held in mind'' the speed or direction of motion in a moving-dots display. Univariate analyses showed elevated delay-period activation of voxels in the prefrontal cortex, in keeping with many working-memory studies. From this activation, however, it was impossible to tell what kind of information (speed or direction) was held in memory. Voxels in different parts of visual cortex did not show elevated delay-period activity in the univariate analysis, but did reliably code information about the speed and direction of motion. Such studies illustrate that different regions can carry completely different kinds of task-related activation, each of which might be invisible depending upon the analysis method.

Third, there is increasing evidence, even from univariate methods, for substantial variability across participants in how information is localized. As one example, \citeA{feredoes_tononi_postle07} conducted univariate analyses of delay-period activation in a working memory task, at both a group level and in individual subjects. In keeping with classic findings, the group-level analysis revealed reliable delay-period activation in prefrontal cortex. The effect size in individual subjects was, however, quite small---the effect was reliable because, like the systematic I/O units in our simulation, it was localized consistently across subjects. Within individual participants, much larger effects were observed in parietal cortex, but because these varied in location across subjects they were not identified in the group level analysis. Importantly, the authors subsequently applied transcranial magnetic stimulation (TMA) to either the common prefrontal region identified in the group analysis or the individualized parietal-lobe peak while participants performed the same working memory task. Application to the individual, variably localized parietal lobe region produced a greater effect on task performance than did application to the shared prefrontal hotspot, suggesting that the individualized results provided a better indication of which regions were importantly contributing to the task.

Together, these empirical findings suggest that, in many cognitive domains, the PDP assumptions may not be far off the mark, notwithstanding the many important findings that have arisen from application of univariate methods. Moreover, regardless of one's stance on the validity of these assumptions specifically, the evidence clearly indicates that different statistical methods can lead to very different conclusions about how the brain represents and processes information. This observation leads to the second question we wish to consider.

{\em Which is the best method to use?} We pose this question somewhat facetiously---it is not our intention to advocate for one method to the exclusion of others! Rather, the point we wish to emphasize is that each statistical method is closely associated with an implicit hypothesis about what matters in neural signal. The question of what matters---what ``makes'' a pattern of activation over neurons a mental representation---is itself a central question, maybe the central question, of cognitive neuroscience. Each of the approaches we have considered begins with an implicit hypothesis about the answer to this question. The univariate contrast method begins with the hypothesis that consistency in location and neural response across individuals is what really matters to the discovery of representation. Searchlight begins with the hypothesis that what matters is the similarity of evoked responses over neural populations within a particular contiguous region of cortex. {\lasso} begins with the hypothesis that representations are sparse, are not highly redundant, and can be localized any which way. Ridge regression begins with the hypothesis that representations {\em are} highly redundant, but still can be localized any which way.

Like any hypothesis, each of these is potentially useful in guiding discovery, but is also subject to empirical assessment. From this point of view, brain imaging might best proceed, not by choosing the ``best'' method, but by comparing and contrasting the results obtained across different methods. With a good understanding of the underlying assumptions each method adopts, and consequently of its blind spots, one may arrive at a better understanding of the neural code than any method individually provides. In the current work, for instance, the univariate contrast method excelled at identifying systematic I/O units but failed to find SH units. SOS {\lasso} showed the reverse pattern. The juxtaposition of the results provides a fuller picture of how the network represents domain structure than does either method on its own. One could imagine conducting a univariate contrast analysis, masking out all voxels that show reliable effects, then conducting a whole-brain multivariate analysis on the remaining voxels to assess if there exists a distributed code. Likewise we have noted that {\soslasso} will only out-perform individual {\lasso} if there is some cross-subject consistency in how representations are anatomically situated. Thus the contrast of classifier performance for {\lasso} and {\soslasso} on the same data provides a direct indication of whether such consistency exists. In general, it is not difficult to imagine how the different methods could be used in combination to better understand the neural basis of mental representations.

It is also worth noting, however, that the hypotheses underlying these different methods, like any hypothesis, must also play a coherent role within a broader theoretical account of the mechanisms that support the behavior of interest. In the case of the univariate contrast method, for instance, it is not generally sufficient simply to state that what matters is consistency in location and neural response across subjects. One wants some broader explanation as to {\em why} cross-subject consistency is an important indicator of the underlying representation. This is where we feel the connection to the PDP framework for cognition offers some real utility. The representational assumptions the approach adopts are not stipulated arbitrarily. They arise from a coherent set of principles stemming from neuroscience, computer science, and cognitive psychology, that have proven useful for understanding a very broad range of behaviors in developing, healthy, and disordered populations. The hypothesis that mental representations are distributed in the particular way we have explored is, in this sense, deeply theoretically motivated. Statistical methods that begin with this hypothesis thus have a built-in theoretical justification that other methods may lack. That is, there already exist good answers to the question of why we should look for distributed representations in brain activity. What has been lacking, and what methods like {\soslasso} can offer, is a good understanding of {\em how} to look.

 
